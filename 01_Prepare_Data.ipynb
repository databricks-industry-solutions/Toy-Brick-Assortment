{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d53e98-40ea-48b8-b20c-c1ca1d293b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The purpose of this notebook is to introduce optimization concepts that are used in assortment optimization.  The concepts will be introduced via a LEGO part optimization example. It was developed on the  **Databricks 15.4 LTS (Scala 2.12, Spark 3.5.0)** runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3f307d-4d22-4c7f-8580-af2442ecbd2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LEGO part optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60447c9d-646f-4888-a42c-7e865f074539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Introduction \n",
    "\n",
    "Welcome to this tutorial on mathematical optimization, where we will dive into the core optimization components using a practical and fun example with LEGO datasets. \n",
    "\n",
    "Imagine you have a number of LEGO parts and you want to figure out which complete LEGO sets can be constructed from those parts.  We will explore how mathematical optimization can help us maximize the value of the sets built, considering the available parts.  \n",
    "\n",
    "As an example, suppose you have a collection of the following parts:\n",
    "\n",
    "- 1 red 2x4 brick\n",
    "- 2 blue 2x4 bricks\n",
    "- 3 green 1x1 bricks\n",
    "- 1 yellow 2x2 brick\n",
    "\n",
    "Assume you have instruction manuals for the following LEGO sets, with each set requiring the following specific parts:\n",
    "\n",
    "- Set A: 1 red 2x4 brick, 2 blue 2x4 bricks, 1 green 1x1 brick.\n",
    "- Set B: 1 red 2x4 brick, 1 blue 2x4 brick, 1 yellow 2x2 brick.\n",
    "- Set C: 2 green 1x1 bricks, 1 yellow 2x2 brick.\n",
    "\n",
    "By trial and error, we can determine that the 7 available parts can be used to create Set A and Set C, without leaving any part unused.  What if we have thousands of parts and hundreds of sets? How can we create an approach to solve this exercise more systematically?  \n",
    "\n",
    "This problem is a great introduction to optimization techniques. Unlike classical machine learning that predicts a future outcome based on current state variables, optimization helps the decision-makers to identify the set of actions required to best achieve a particular outcome. These challenges are especially prevalent in the retail industry. Some relevant use cases include:\n",
    "\n",
    "* **Inventory Optimization**: What is the right level of inventory to hold at various locations to ensure customer demand is easily satisfied but to also minimize locked up capital?\n",
    "* **Supply Chain**: What is the right network design for us to ensure goods are distributed at the right frequency at the right locations while minimizing storage and transportation costs?\n",
    "* **Product Assortment**: What is the right mix of products to provide customers to satisfy customer needs and maximize profits but also dealing with limited shelf space?\n",
    "\n",
    "There are significant parallels between the fun LEGO example we describe above and Product Assortment in the retail industry. For example:\n",
    "* **Constraints on availability**: We have limited LEGO bricks available, similar to retail stores where shelf space and product inventory is limited\n",
    "* **Multiple choices**: We have to choose which LEGO sets to build with the bricks available, similar to retail stores which have to determine which products to display from a larger selection of potential SKUs\n",
    "* **Substitution effects**: There are multiple LEGO sets of a similar theme(for example Star Wars). Similarly, there are multiple products of a similar type usually competing for shelf space (example: Navel Oranges vs Cara Cara Oranges)\n",
    "* **Complementary products**: We could potentially purchase loose LEGO bricks to make new sets. Similarly, retailers can decide to stock new high value SKUs that increase overall purchases from a customer (example: stocking cookies might increase milk purchase from customers)\n",
    "\n",
    "Mathematical optimization is a powerful tool to address the Product Assortment challenges in the retail industry. Our goal in these set of notebooks is to provide you with a gentle introduction to the basics of mathematical optimization, mathematical formulations for optimization, and their implementation and solution, all powered by the Databricks Data Intelligence Platform. We will use the LEGO example as the underlying problem to introduce these concepts in a fun and accessible way. Whether you are new to optimization or looking to reinforce your understanding, this tutorial provides a clear path to learning and applying mathematical programming concepts. The word programming in this context is used as a synonym for the word optimization.\n",
    "\n",
    "We will use [Gurobi](https://www.gurobi.com/) as the optimization solver.  Gurobi is a power solver that can be used to solve a wide range of mathematical programming (optimization) problems, including linear programming (LP), mixed-integer programming (MIP), quadratic programming (QP), and more. It is designed to handle large-scale problems efficiently and can be easily integrated with your applications with its APIs in multiple languages including Python. In this tutorial, we will adjust input data and keep the optimization model small to ensure it can be solved without a commercial Gurobi license. For large scale applications, procuring a commercial license from Gurobi may be necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4836c8c0-1444-465b-849b-154656fec484",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In this notebook, we will access the datasets used in this exercise and assemble a subset that is tailored to the specific problems we wish to tackle. Please note that the series continues with two notebooks:\n",
    "\n",
    "- <a href=\"$./Optimization_Model\">Optimization_Model</a>: describes the application of an optimization model on a small example to introduce key concepts and build intuition for these methods.\n",
    "- <a href=\"$./Optimization_Model_Large\">Optimization_Model_Large</a>: describes the application of the same model on a large scale dataset to showcase the utility of optimization modelling for real-world problems.\n",
    "\n",
    "The input datasets for both notebooks above are assembled in the following cells. Please note that all datasets with a `_large` suffix are utilized in the <a href=\"$./Optimization_Model_Large\">Optimization_Model_Large</a> notebook, while the relatively smaller datasets without the suffix are used in the <a href=\"$./Optimization_Model\">Optimization_Model</a> notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2faa302-c06d-4bc9-a693-637be52cc34d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn  # Importing the functions module from pyspark.sql to use built-in SQL functions\n",
    "from pyspark.sql.functions import col # col() is a highly useful function that allows you to reference columns in a Spark DataFrame.\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# functions needed to perturb the parts set\n",
    "from pyspark.sql.functions import rand, when, lit, floor\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e237fdd1-3fa7-4ed4-b3e0-ab37ac1161bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Access the Lego Dataset\n",
    "\n",
    "The LEGO dataset is obtained from Kaggle [LEGO Database](https://www.kaggle.com/datasets/rtatman/lego-database), which is originally from [Rebrickable](https://rebrickable.com/help/lego-database/).  It consists of eight comma-separated files.  For now, we will create the containers needed to hold all these items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae17f1b-7985-45e7-a541-0680cc9d18ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a widget to accept a catalog name\n",
    "dbutils.widgets.text('catalog_name','catalog_name','Enter catalog name')\n",
    "dbutils.widgets.text('schema_name','schema_name','Enter schema name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba4de15-c68a-4450-83c2-c650f6be5683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the catalog name\n",
    "catalog_name = dbutils.widgets.get('catalog_name')\n",
    "schema_name = dbutils.widgets.get('schema_name')\n",
    "print(catalog_name + \"/\" + schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6b5ca5-3077-4a34-bb15-b2dd61797fe8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Containers for Data Objects"
    }
   },
   "outputs": [],
   "source": [
    "# create catalog dynamically\n",
    "catalog_sql = f\"USE CATALOG {catalog_name};\"\n",
    "schema_sql = f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name};\"\n",
    "volume_sql = f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.csv_inputs;\"\n",
    "\n",
    "# Execute the commands\n",
    "spark.sql(catalog_sql)\n",
    "spark.sql(schema_sql)\n",
    "spark.sql(volume_sql)\n",
    "\n",
    "print(f\"Catalog '{catalog_name}', schema '{schema_name}', and volume 'csv_inputs' are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f7edab-2a36-46c1-b540-135be13c1490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using the Catalog Explorer, navigate to the specified catalog. </p> \n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/legos_catalog_explorer_catalog.PNG' width=80%></p>\n",
    "\n",
    "Click on the *legos* schema, select the *Volumes* tab and click on the *csv_inputs* volume.  Click the *Upload Files* button in the upper right-hand corner of the screen and to bring up the file upload dialog box.\n",
    "</p>\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/logos_catalog_explorer_volumes.PNG' width=80%></p>\n",
    "\n",
    "Download the eight CSV files that make up this dataset to your local system, unzip any of the larger files that Kaggle may have automatically compressed and drag and drop the CSV files to the volume upload dialog in order to upload into your Databricks environment.  (Please note, this could be done programmatically but to avoid juggling Kaggle keys, we have elected to describe a manual process.)\n",
    "</p>\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/legos_catalog_explorer_upload_complete.PNG' width=80%></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb65e6f-daf2-4d38-9381-b4f4b181ceaa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify CSV Files in Volume"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path dynamically using the catalog name\n",
    "path = f\"/Volumes/{catalog_name}/{schema_name}/csv_inputs/\"\n",
    "\n",
    "# List the contents of the directory\n",
    "files = dbutils.fs.ls(path)\n",
    "\n",
    "# Print the list of files in the directory\n",
    "for file in files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c077e9-3abc-4cc2-a594-d8185a94c53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can now read each file into a table to make it more accessible in later steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99315181-e6e7-47dd-b95f-cf4a537c7f88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for filename in dbutils.fs.ls(f\"/Volumes/{catalog_name}/{schema_name}/csv_inputs/\"):\n",
    "    \n",
    "    # if this is a CSV input file\n",
    "    if filename.name.endswith('.csv'):\n",
    "\n",
    "        # what table are we creating\n",
    "        dataset = filename.name[:-4]  # ignore the '.csv' suffix\n",
    "        print(dataset)\n",
    "        # read CSV and write as delta table\n",
    "        (\n",
    "            spark\n",
    "                .read\n",
    "                    .csv(\n",
    "                        path=filename.path,\n",
    "                        sep=',',\n",
    "                        header=True,\n",
    "                        inferSchema=True\n",
    "                    )\n",
    "                .write\n",
    "                    .format('delta')\n",
    "                    .mode('overwrite')\n",
    "                    .option('overwriteSchema','true')\n",
    "                    .saveAsTable(f\"{catalog_name}.{schema_name}.{dataset}\")\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556ffedc-4f07-461a-bc59-4d3ab01641ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Show Tables"
    }
   },
   "outputs": [],
   "source": [
    "query = f\"SHOW TABLES IN {catalog_name}.{schema_name};\"\n",
    "\n",
    "tables = spark.sql(query)\n",
    "tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d68aa16-14b7-46bc-b27e-d6407a903af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Step 2: Create Datasets\n",
    "\n",
    "Let's pretend we have all the parts from the following four Lego sets:\n",
    "\n",
    "* [75160-1: U-wing](https://www.lego.com/en-us/product/u-wing-microfighter-75160)\n",
    "* [75162-1: Y-wing](https://www.lego.com/en-us/product/y-wing-microfighter-75162)\n",
    "* [75168-1: Yoda's Jedi Starfighter](https://www.lego.com/en-us/product/yoda-s-jedi-starfighter-75168)\n",
    "* [75170-1: The Phantom](https://www.lego.com/en-us/product/the-phantom-75170)\n",
    "\n",
    "These LEGO parts are used to construct our pile of \"loose parts\".  After we construct and solve the optimization model, we hope to see that these individual LEGO parts can be used to construct the four sets above.  \n",
    "\n",
    "As a separate exercise, you can use other approaches to generate a list of LEGO parts and see which sets can be constructed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b939fe-4628-44fe-bd18-a3cb3b8f7a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data from tables stored earlier\n",
    "inventories = spark.table(f\"{catalog_name}.{schema_name}.inventories\")\n",
    "themes = spark.table(f\"{catalog_name}.{schema_name}.themes\")\n",
    "colors = spark.table(f\"{catalog_name}.{schema_name}.colors\")\n",
    "sets = spark.table(f\"{catalog_name}.{schema_name}.sets\")\n",
    "inventory_parts = spark.table(f\"{catalog_name}.{schema_name}.inventory_parts\")\n",
    "parts = spark.table(f\"{catalog_name}.{schema_name}.parts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c43587-df58-4a5d-8e67-32ade38e5afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Note \n",
    "\"sets\" table contains over 10k LEGO sets.  We will reduce LEGO set options to keep the input data small, so that we can meet model size limit for Gurobi's free license and execute the <a href=\"$./Optimization_Model\">Optimization_Model</a> notebook without a commercial Gurobi license. We will create reduced datasets below. \n",
    "\n",
    "Please note that the model solved in the notebook <a href=\"$./Optimization_Model_Large\">Optimization_Model_Large</a> does exceed the size limit for Gurobi's free license. To execute this notebook, you will need to procure an appropriate license from [Gurobi](https://www.gurobi.com/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a056ba5d-174b-4df8-b618-0a97fcf59610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the 'themes' table to find the 'Star Wars' themes to keep a smaller number of LEGO sets\n",
    "filtered_theme_id = (\n",
    "    themes\n",
    "    .filter(col('name').contains(\"Star Wars\"))\n",
    "    .select(\"id\")  # Select the 'id' column\n",
    "    .toPandas()['id']  # Convert to pandas DataFrame and select the 'id' column\n",
    "    .tolist()  # Convert to a Python list\n",
    ")\n",
    "\n",
    "# More attempt to reduce data size - assume we only want to consider building sets that meet the following criteria\n",
    "sets = (\n",
    "    sets\n",
    "        .filter(col(\"theme_id\").isin(filtered_theme_id))\n",
    "        .filter(\"year = 2017\") # focus on just 2017 variants\n",
    "        .filter(\"num_parts between 70 and 399\") # keep part sizes manageable\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30557e68-fdfa-461a-9963-3cc27cc33fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We need a table that has both unique LEGO set ID and part ID that defines the number of each part in each LEGO set.  To do that, we need to join the \"inventories\" and \"inventory_parts\" tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f50f26-0dd3-4da5-af88-696e2c21de53",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1737396091443}",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "part_in_set = ( \n",
    "    inventories\n",
    "        .join(inventory_parts, on=inventories['id']==inventory_parts['inventory_id'], how='inner')\n",
    "        .join(sets, on='set_num', how='leftsemi')\n",
    "        .groupBy(['set_num','part_num','color_id'])\n",
    "            .agg(\n",
    "                fn.sum('quantity').alias('quantity')\n",
    "            )\n",
    "    )\n",
    "\n",
    "display(part_in_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80f9c71-ddec-4d86-8f15-969ad44ee52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a dataset `parts_avail` for available pieces to be allocated to LEGO sets (this is the set of \"loose\" pieces we have on hand).\n",
    "\n",
    "We perturb the available inventory by removing one part common to two sets. This will provide a test for our optimization model: which LEGO sets will it choose to build given it cannot build all four anymore due to the missing part?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f4422f-494e-4d28-b799-478b522facc8",
     "showTitle": true,
     "tableResultSettingsMap": {
      "1": {
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1737403255787}",
       "tableResultIndex": 1
      }
     },
     "title": "Select Four Lego Sets"
    }
   },
   "outputs": [],
   "source": [
    "my_sets = sets.filter(\"set_num IN ('75160-1', '75162-1', '75168-1', '75170-1')\")\n",
    "\n",
    "display(my_sets)\n",
    "\n",
    "parts_avail = ( \n",
    "    part_in_set\n",
    "        .join(my_sets, on='set_num', how='inner')\n",
    "        .groupBy(['part_num','color_id'])\n",
    "            .agg(\n",
    "                fn.sum('quantity').alias('quantity')\n",
    "            )\n",
    "    )\n",
    "\n",
    "# Perturb inventory by removing  1 part common to two sets\n",
    "parts_avail = parts_avail.withColumn(\n",
    "    'quantity',\n",
    "    fn.when(\n",
    "        (parts_avail['part_num'] == '3020') & (parts_avail['color_id'] == 71),\n",
    "        parts_avail['quantity'] - 1\n",
    "    ).otherwise(parts_avail['quantity'])\n",
    ")\n",
    "\n",
    "display(parts_avail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03fd1610-8b52-4060-bc66-510d4ead3675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following cell will create a reduced dataset to retain only relevant data (i.e. we will only keep a LEGO set, part, or color in the tables if they are relevant options within the reduced LEGO sets created above).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba2dac2-0dca-4417-bc4f-43855111412e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "relevant_set = ( \n",
    "    part_in_set\n",
    "        .join(parts_avail, on=part_in_set['part_num']==parts_avail['part_num'], how='inner')\n",
    "    ).select(part_in_set[\"*\"])\n",
    "\n",
    "display(relevant_set)\n",
    "\n",
    "sets = (\n",
    "    sets\n",
    "        .join(relevant_set.select(\"set_num\"), on=\"set_num\", how=\"inner\")\n",
    "        .select(sets[\"*\"])\n",
    ").distinct()\n",
    "\n",
    "\n",
    "display(sets)\n",
    "\n",
    "parts = (parts\n",
    "         .join(part_in_set.select(\"part_num\"), on=\"part_num\", how=\"inner\")\n",
    "         .select(parts[\"*\"])\n",
    ").distinct()\n",
    "\n",
    "display(parts)\n",
    "\n",
    "colors = (\n",
    "    colors\n",
    "    .join(part_in_set.select(\"color_id\"), \n",
    "          colors[\"id\"] == part_in_set[\"color_id\"], \n",
    "          \"inner\")\n",
    "    .select(colors[\"*\"])\n",
    ").distinct()\n",
    "\n",
    "display(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebff2254-9dc6-4077-8c13-4f1c0b5c0877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Step 3: Persist Data for Optimization Steps\n",
    "\n",
    "We have now created all datasets that will be utilized in the <a href=\"$./Optimization_Model\">Optimization_Model</a> notebook! Before moving on, it would be helpful to persist newly created tables and update existing tables so we don't have to duplicate data processing steps in the Optimization_Model notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24ceb31c-5453-401b-a4d9-2f9d2a244a14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Persist Data on Selected Sets"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    my_sets\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.my_sets\")    \n",
    ")\n",
    "\n",
    "(\n",
    "    relevant_set\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.relevant_set\")\n",
    ")\n",
    "\n",
    "(\n",
    "    sets\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.sets\")\n",
    "    )\n",
    "\n",
    "(\n",
    "    parts\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.parts\")\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    colors\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.colors\")\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    parts_avail\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.parts_avail\")\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    part_in_set\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.part_in_set\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ac0cef-02dc-4914-a624-1f925ee40e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Step 4: Prepare Larger Inventory Datasets\n",
    "\n",
    "In steps 4 and 5 we will repeat the steps above, expanding the initial number of LEGO sets to more than 200 Star Wars themed sets.\n",
    "These datasets will be utilized in notebook <a href=\"$./Optimization_Model_Large\">Optimization_Model_Large</a> notebook to define a more challenging inventory to be optimized. \n",
    "\n",
    "First, re-read datasets from source and store them as tables with `_large` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef41b043-a33a-4cd8-be83-401644ab5748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for filename in dbutils.fs.ls(f\"/Volumes/{catalog_name}/{schema_name}/csv_inputs/\"):\n",
    "    \n",
    "    # if this is a CSV input file\n",
    "    if filename.name.endswith('.csv'):\n",
    "\n",
    "        # what table are we creating\n",
    "        dataset = filename.name[:-4]  # ignore the '.csv' suffix\n",
    "        print(dataset)\n",
    "        # read CSV and write as delta table\n",
    "        (\n",
    "            spark\n",
    "                .read\n",
    "                    .csv(\n",
    "                        path=filename.path,\n",
    "                        sep=',',\n",
    "                        header=True,\n",
    "                        inferSchema=True\n",
    "                    )\n",
    "                .write\n",
    "                    .format('delta')\n",
    "                    .mode('overwrite')\n",
    "                    .option('overwriteSchema','true')\n",
    "                    .saveAsTable(f\"{catalog_name}.{schema_name}.{dataset}_large\")\n",
    "\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62efbbe-b6dd-40b4-92d4-d29fd84d4a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data from tables stored earlier\n",
    "inventories_large = spark.table(f\"{catalog_name}.{schema_name}.inventories_large\")\n",
    "themes_large = spark.table(f\"{catalog_name}.{schema_name}.themes_large\")\n",
    "colors_large = spark.table(f\"{catalog_name}.{schema_name}.colors_large\")\n",
    "sets_large = spark.table(f\"{catalog_name}.{schema_name}.sets_large\")\n",
    "inventory_parts_large = spark.table(f\"{catalog_name}.{schema_name}.inventory_parts_large\")\n",
    "parts_large = spark.table(f\"{catalog_name}.{schema_name}.parts_large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb5c66d-af7b-45ae-95f4-4f104fdeb064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can now regenerate our initial collection by selecting all Star Wars sets released since the year 2000. The `filter_theme_id` can be reused as it already references the desire theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb18c7f-3f82-4519-bcd9-818021862cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the sets_large to Star Wars theme, released after 2000\n",
    "sets_large = (\n",
    "    sets_large\n",
    "    .filter(col(\"theme_id\").isin(filtered_theme_id))\n",
    "    .filter(col(\"year\") > 2000)\n",
    "    .filter(\"num_parts between 70 and 400\") # keep part sizes manageable\n",
    ")\n",
    "\n",
    "display(sets_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d547d5f-63d8-4d68-b948-32d7e3e91a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We need a new table that has both unique LEGO set ID and part ID that defines the number of each part in each LEGO set in the large inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b7a5f8-28d6-4338-80ff-20a664f3254f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "part_in_set_large = ( \n",
    "    inventories_large\n",
    "        .join(inventory_parts_large, on=inventories_large.id == inventory_parts_large.inventory_id, how='inner')\n",
    "        .join(sets_large, on='set_num', how='leftsemi')\n",
    "        .groupBy(['set_num','part_num','color_id'])\n",
    "            .agg(\n",
    "                fn.sum('quantity').alias('quantity')\n",
    "            )\n",
    "    )\n",
    "\n",
    "display(part_in_set_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2c01f0-b4bf-427f-a7fe-44d37065a24f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Create a dataset `parts_avail_large` for available pieces to be allocated to LEGO sets (this is the set of \"loose\" pieces we have on hand). We use a randomized strategy to:\n",
    "* Add or remove single pieces regardless of their quantity\n",
    "* Increase or decrease significantly the total for common pieces\n",
    "\n",
    "These changes will define an inventory such that some of the initial sets will be infeasible to build as some crucial single pieces will not be available. The common pieces modification will make it more likely that there exist compromises in which sets to build.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d6fc16-cb17-48b8-9b27-a7a899b17d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parts_avail_large = ( \n",
    "    part_in_set_large\n",
    "        .join(sets_large, on='set_num', how='inner')\n",
    "        .groupBy(['part_num', 'color_id'])\n",
    "            .agg(\n",
    "                fn.sum('quantity').alias('quantity')\n",
    "            )\n",
    "    )\n",
    "\n",
    "# Perturb the quantity of each part in the set\n",
    "r_seed = 13\n",
    "parts_avail_large = parts_avail_large.withColumn(\n",
    "    'perturbation',\n",
    "    when(rand(seed=r_seed) < 0.8, lit(0))\n",
    "    .when(rand(seed=r_seed) < 0.9, lit(1))\n",
    "    .otherwise(lit(-1))\n",
    ")\n",
    "\n",
    "# Make some multiplication to large quantity parts\n",
    "parts_avail_large = parts_avail_large.withColumn(\n",
    "    'perturbation_multi',\n",
    "    when((rand(seed=r_seed) < 0.3) & (col(\"quantity\") > 60) , lit(1.5))\n",
    "    .when((rand(seed=r_seed) < 0.6) & (col(\"quantity\") > 60), lit(0.5))\n",
    "    .otherwise(lit(1))\n",
    ")\n",
    "\n",
    "parts_avail_large = parts_avail_large.withColumn('quantity', col('quantity') + col('perturbation'))\n",
    "parts_avail_large = parts_avail_large.withColumn('quantity', (col('quantity') * col('perturbation_multi')).cast(IntegerType()))\n",
    "\n",
    "display(parts_avail_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1da2bb2-a1c9-4a6b-a682-818d1377e357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Similarly to above, we generate the other datasets required to describe the large inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78183016-d06b-4731-ac5c-04da41e5a462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "relevant_set_large = ( \n",
    "    part_in_set_large\n",
    "        .join(parts_avail_large, on=part_in_set_large['part_num']==parts_avail_large['part_num'], how='inner')\n",
    "    ).select(part_in_set_large[\"*\"])\n",
    "\n",
    "display(relevant_set_large)\n",
    "\n",
    "sets_large = (\n",
    "    sets_large\n",
    "        .join(relevant_set_large.select(\"set_num\"), on=\"set_num\", how=\"inner\")\n",
    "        .select(sets_large[\"*\"])\n",
    ").distinct()\n",
    "\n",
    "\n",
    "display(sets_large)\n",
    "\n",
    "parts_large = (parts_large\n",
    "         .join(part_in_set_large.select(\"part_num\"), on=\"part_num\", how=\"inner\")\n",
    "         .select(parts_large[\"*\"])\n",
    ").distinct()\n",
    "\n",
    "display(parts_large)\n",
    "\n",
    "colors_large = (\n",
    "    colors_large\n",
    "    .join(part_in_set_large.select(\"color_id\"), \n",
    "          colors_large[\"id\"] == part_in_set_large[\"color_id\"], \n",
    "          \"inner\")\n",
    "    .select(colors_large[\"*\"])\n",
    ").distinct()\n",
    "\n",
    "display(colors_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e21b62e7-df26-41d5-b599-26bfca4082da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Step 5: Persist Data for Large Inventory Optimization Steps\n",
    "\n",
    "We have now created all datasets that will be utilized in the <a href=\"$./Optimization_Model_Large\">Optimization_Model_Large</a> notebook! Before moving on, it would be helpful to persist newly created tables and update existing tables so we don't have to duplicate data processing steps in the Optimization_Model_Large notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ff8ad7-7c42-4453-9c23-6e1a8678174b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    relevant_set_large\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.relevant_set_large\")\n",
    ")\n",
    "\n",
    "(\n",
    "    sets_large\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.sets_large\")\n",
    "    )\n",
    "\n",
    "(\n",
    "    parts_large\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.parts_large\")\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    colors_large\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.colors_large\")\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    parts_avail_large\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.parts_avail_large\")\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    part_in_set_large\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema','true')\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_name}.part_in_set_large\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9c03f8-52f5-4201-b30e-850fe659d46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "catalog_name",
      "width": 183
     }
    ]
   },
   "notebookName": "01_Prepare_Data",
   "widgets": {
    "catalog_name": {
     "currentValue": "prod_catalog",
     "nuid": "09dffdb1-6df4-4485-be75-441fb4ec48c0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "catalog_name",
      "label": "Enter catalog name",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "catalog_name",
      "label": "Enter catalog name",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "legos",
     "nuid": "1c983774-b26f-48c3-9ffc-c1122b5192ee",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "schema_name",
      "label": "Enter schema name",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "schema_name",
      "label": "Enter schema name",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
